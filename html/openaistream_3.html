<div class="container">

<table style="width: 100%;"><tr>
<td>audio</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>audio Class</h2>

<h3>Description</h3>

<p>To turn audio into text or text into audio
</p>


<h3>Super class</h3>

<p><code>openaistream::openai</code> -&gt; <code>audio</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-audio-speech"><code>audio$speech()</code></a>
</p>
</li>
<li> <p><a href="#method-audio-transcription"><code>audio$transcription()</code></a>
</p>
</li>
<li> <p><a href="#method-audio-translation"><code>audio$translation()</code></a>
</p>
</li>
<li> <p><a href="#method-audio-clone"><code>audio$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="openaistream" data-topic="openai" data-id="initialize"><a href="../../openaistream/html/openai.html#method-openai-initialize"><code>openaistream::openai$initialize()</code></a></span></li>
</ul></details><hr>
<a id="method-audio-speech"></a>



<h4>Method <code>speech()</code>
</h4>

<p>Generates audio from the input text.
</p>


<h5>Usage</h5>

<div class="r"><pre>audio$speech(
  model = "tts-1",
  input,
  voice = "alloy",
  stream = F,
  num = 100,
  ...,
  verbosity = 0
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt>
<dd>
<p>character Required. One of the available TTS models: tts-1 or tts-1-hd</p>
</dd>
<dt><code>input</code></dt>
<dd>
<p>character Required. The text to generate audio for. The maximum length is 4096 characters.</p>
</dd>
<dt><code>voice</code></dt>
<dd>
<p>character Required. The voice to use when generating the audio. Supported voices are alloy, echo, fable, onyx, nova, and shimmer.</p>
</dd>
<dt><code>stream</code></dt>
<dd>
<p>logical. Using the stream call, it will return raw data of the specified length,
which can be saved in the set format such as mp3, etc. For details, please see the examples.</p>
</dd>
<dt><code>num</code></dt>
<dd>
<p>The num parameter controls the number of raw entries returned by a stream in one go.
Note that this is different from the n parameter, which specifies the number of results returned.
For detailed information on the n parameter, please refer to OpenAI's API documentation.</p>
</dd>
<dt><code>...</code></dt>
<dd>
<p>Additional parameters as required by the OpenAI API.For example:response_format;speed....</p>
</dd>
<dt><code>verbosity</code></dt>
<dd>
<p>numeric. Verbosity level for the API call(0:no output;1:show headers;
2:show headers and bodies;3: show headers, bodies, and curl status messages.).</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>The audio file content.
</p>


<hr>
<a id="method-audio-transcription"></a>



<h4>Method <code>transcription()</code>
</h4>

<p>Transcribes audio into the input language.
</p>


<h5>Usage</h5>

<div class="r"><pre>audio$transcription(path, model = "whisper-1", ..., verbosity = 0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>path</code></dt>
<dd>
<p>character Required. The audio file object (not file name) to transcribe,
in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p>
</dd>
<dt><code>model</code></dt>
<dd>
<p>character Required. ID of the model to use. Only whisper-1 is currently available.</p>
</dd>
<dt><code>...</code></dt>
<dd>
<p>Additional parameters as required by the OpenAI API.For example:language;prompt;response_format;temperature....</p>
</dd>
<dt><code>verbosity</code></dt>
<dd>
<p>numeric. Verbosity level for the API call(0:no output;1:show headers;
2:show headers and bodies;3: show headers, bodies, and curl status messages.).</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>The transcribed text.
</p>


<hr>
<a id="method-audio-translation"></a>



<h4>Method <code>translation()</code>
</h4>

<p>Translates audio into English.
</p>


<h5>Usage</h5>

<div class="r"><pre>audio$translation(path, model = "whisper-1", ..., verbosity = 0)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>path</code></dt>
<dd>
<p>character Required. The audio file object (not file name) to transcribe,
in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.</p>
</dd>
<dt><code>model</code></dt>
<dd>
<p>character Required. ID of the model to use. Only whisper-1 is currently available.</p>
</dd>
<dt><code>...</code></dt>
<dd>
<p>Additional parameters as required by the OpenAI API.For example:prompt;response_format;temperature....</p>
</dd>
<dt><code>verbosity</code></dt>
<dd>
<p>numeric. Verbosity level for the API call(0:no output;1:show headers;
2:show headers and bodies;3: show headers, bodies, and curl status messages.).</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>The transcribed text.
</p>


<hr>
<a id="method-audio-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>audio$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




</div>