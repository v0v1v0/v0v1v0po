<div class="container">

<table style="width: 100%;"><tr>
<td>multinomial_ml</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multinomial Machine Learning</h2>

<h3>Description</h3>

<p>Estimation strategy to estimate conditional choice probabilities for ordered non-numeric outcomes.
</p>


<h3>Usage</h3>

<pre><code class="language-R">multinomial_ml(Y = NULL, X = NULL, learner = "forest", scale = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Outcome vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learner</code></td>
<td>
<p>String, either <code>"forest"</code> or <code>"l1"</code>. Selects the base learner to estimate each expectation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>Logical, whether to scale the covariates. Ignored if <code>learner</code> is not <code>"l1"</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Multinomial machine learning expresses conditional choice probabilities as expectations of binary variables:
</p>
<p style="text-align: center;"><code class="reqn">p_m \left( X_i \right) = \mathbb{E} \left[ 1 \left( Y_i = m \right) | X_i \right]</code>
</p>

<p>This allows us to estimate each expectation separately using any regression algorithm to get an estimate of conditional probabilities.<br></p>
<p><code>multinomial_ml</code> combines this strategy with either regression forests or penalized logistic regressions with an L1 penalty,
according to the user-specified parameter <code>learner</code>.<br></p>
<p>If <code>learner == "l1"</code>, the penalty parameters are chosen via 10-fold cross-validation 
and <code>model.matrix</code> is used to handle non-numeric covariates. Additionally, if <code>scale == TRUE</code>, the covariates are scaled to 
have zero mean and unit variance.
</p>


<h3>Value</h3>

<p>Object of class <code>mml</code>.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul><li>
<p> Di Francesco, R. (2023). Ordered Correlation Forest. arXiv preprint <a href="https://arxiv.org/abs/2309.08755">arXiv:2309.08755</a>.
</p>
</li></ul>
<h3>See Also</h3>

<p><code>ordered_ml</code>, <code>ocf</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Generate synthetic data.
set.seed(1986)

data &lt;- generate_ordered_data(100)
sample &lt;- data$sample
Y &lt;- sample$Y
X &lt;- sample[, -1]

## Training-test split.
train_idx &lt;- sample(seq_len(length(Y)), floor(length(Y) * 0.5))

Y_tr &lt;- Y[train_idx]
X_tr &lt;- X[train_idx, ]

Y_test &lt;- Y[-train_idx]
X_test &lt;- X[-train_idx, ]

## Fit multinomial machine learning on training sample using two different learners.
multinomial_forest &lt;- multinomial_ml(Y_tr, X_tr, learner = "forest")
multinomial_l1 &lt;- multinomial_ml(Y_tr, X_tr, learner = "l1")

## Predict out of sample.
predictions_forest &lt;- predict(multinomial_forest, X_test)
predictions_l1 &lt;- predict(multinomial_l1, X_test)

## Compare predictions.
cbind(head(predictions_forest), head(predictions_l1))

</code></pre>


</div>