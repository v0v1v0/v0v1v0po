<div class="container">

<table style="width: 100%;"><tr>
<td>FTRL</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Implementation of FTRL (Follow The Regularized Leader)</h2>

<h3>Description</h3>

<p>FTRL (Shalev-Shwartz and Singer 2007) and Chap. 5 of (Hazan 2019) is the online counterpart of empirical risk minimization. 
It is a family of aggregation rules (including OGD) that uses at any time the empirical risk
minimizer so far with an additional regularization. The online optimization can be performed
on any bounded convex set that can be expressed with equality or inequality constraints. 
Note that this method is still under development and a beta version.
</p>


<h3>Usage</h3>

<pre><code class="language-R">FTRL(
  y,
  experts,
  eta = NULL,
  fun_reg = NULL,
  fun_reg_grad = NULL,
  constr_eq = NULL,
  constr_eq_jac = NULL,
  constr_ineq = NULL,
  constr_ineq_jac = NULL,
  loss.type = list(name = "square"),
  loss.gradient = TRUE,
  w0 = NULL,
  max_iter = 50,
  obj_tol = 0.01,
  training = NULL,
  default = FALSE,
  quiet = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p><code>vector</code>. Real observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>experts</code></td>
<td>
<p><code>matrix</code>. Matrix of experts previsions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p><code>numeric</code>. Regularization parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fun_reg</code></td>
<td>
<p><code>function</code> (NULL). Regularization function to be applied during the optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fun_reg_grad</code></td>
<td>
<p><code>function</code> (NULL). Gradient of the regularization function (to speed up the computations).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constr_eq</code></td>
<td>
<p><code>function</code> (NULL). Constraints (equalities) to be applied during the optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constr_eq_jac</code></td>
<td>
<p><code>function</code> (NULL). Jacobian of the equality constraints (to speed up the computations).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constr_ineq</code></td>
<td>
<p><code>function</code> (NULL). Constraints (inequalities) to be applied during the optimization (... &gt; 0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constr_ineq_jac</code></td>
<td>
<p><code>function</code> (NULL). Jacobian of the inequality constraints (to speed up the computations).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss.type</code></td>
<td>
<p><code>character, list or function</code> ("square").
</p>

<dl>
<dt>character</dt>
<dd>
<p> Name of the loss to be applied ('square', 'absolute', 'percentage', or 'pinball');</p>
</dd>
<dt>list</dt>
<dd>
<p> List with field <code>name</code> equal to the loss name. If using pinball loss, field <code>tau</code> equal to the required quantile in [0,1];</p>
</dd>
<dt>function</dt>
<dd>
<p> A custom loss as a function of two parameters (prediction, label).</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss.gradient</code></td>
<td>
<p><code>boolean, function</code> (TRUE). 
</p>

<dl>
<dt>boolean</dt>
<dd>
<p> If TRUE, the aggregation rule will not be directly applied to the loss function at hand,
but to a gradient version of it. The aggregation rule is then similar to gradient descent aggregation rule. </p>
</dd>
<dt>function</dt>
<dd>
<p> If loss.type is a function, the derivative of the loss in its first component should be provided to be used (it is not automatically 
computed).</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w0</code></td>
<td>
<p><code>numeric</code> (NULL). Vector of initialization for the weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p><code>integer</code> (50). Maximum number of iterations of the optimization algorithm per round.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>obj_tol</code></td>
<td>
<p><code>numeric</code> (1e-2). Tolerance over objective function between two iterations of the optimization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training</code></td>
<td>
<p><code>list</code> (NULL). List of previous parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>default</code></td>
<td>
<p><code>boolean</code> (FALSE). Whether or not to use default parameters for fun_reg, constr_eq, constr_ineq and their grad/jac, 
which values are ALL ignored when TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quiet</code></td>
<td>
<p><code>boolean</code> (FALSE). Whether or not to display progress bars.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>object of class mixture.
</p>


<h3>References</h3>

<p>Hazan E (2019).
“Introduction to online convex optimization.”
<em>arXiv preprint arXiv:1909.05207</em>.<br><br> Shalev-Shwartz S, Singer Y (2007).
“A primal-dual perspective of online learning algorithms.”
<em>Machine Learning</em>, <b>69</b>(2), 115–142.
</p>


</div>