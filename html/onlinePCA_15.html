<div class="container">

<table style="width: 100%;"><tr>
<td>sgapca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Stochastic Gradient Ascent PCA</h2>

<h3>Description</h3>

<p>Online PCA with the SGA algorithm of Oja (1992).</p>


<h3>Usage</h3>

<pre><code class="language-R">sgapca(lambda, U, x, gamma, q = length(lambda), center, 
	type = c("exact", "nn"), sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>optional vector of eigenvalues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>new data vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>vector of gain parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>algorithm implementation: "exact" or "nn" (neural network).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sort</code></td>
<td>
<p>Should the new eigenpairs be sorted?</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The gain vector <code>gamma</code> determines the weight placed on the new data in updating each principal component. The first coefficient of <code>gamma</code> corresponds to the first principal component, etc.. It can be specified as a single positive number (which is recycled by the function) or as a vector of length <code>ncol(U)</code>. For larger values of <code>gamma</code>, more weight is placed on <code>x</code> and less on <code>U</code>. A common choice for (the components of) <code>gamma</code> is of the form <code>c/n</code>, with <code>n</code> the sample size and <code>c</code> a suitable positive constant. <br> 
The Stochastic Gradient Ascent PCA can be implemented exactly or through a neural network. The latter is less accurate but faster.<br>
If <code>sort</code> is TRUE and <code>lambda</code> is not missing, the updated eigenpairs are sorted by decreasing eigenvalue. Otherwise, they are not sorted.  
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table>
<tr style="vertical-align: top;">
<td><code>values</code></td>
<td>
<p>updated eigenvalues or NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vectors</code></td>
<td>
<p>updated principal components.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Oja (1992). Principal components, Minor components, and linear neural networks. <em>Neural Networks.</em>
</p>


<h3>See Also</h3>

<p><code>ghapca</code>, 
<code>snlpca</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Initialization
n &lt;- 1e4  # sample size
n0 &lt;- 5e3 # initial sample size
d &lt;- 10   # number of variables
q &lt;- d # number of PC to compute
x &lt;- matrix(runif(n*d), n, d)
x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of x are close to 1, 2, ..., d
# and the corresponding eigenvectors are close to 
# the canonical basis of R^d

## SGA PCA
xbar &lt;- colMeans(x[1:n0,])
pca &lt;- batchpca(x[1:n0,], q, center=xbar, byrow=TRUE)
for (i in (n0+1):n) {
  xbar &lt;- updateMean(xbar, x[i,], i-1)
  pca &lt;- sgapca(pca$values, pca$vectors, x[i,], 2/i, q, xbar)
}
pca
</code></pre>


</div>