<div class="container">

<table style="width: 100%;"><tr>
<td>snewton</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Safeguarded Newton methods for function minimization using R functions.</h2>

<h3>Description</h3>

<p>These versions of the safeguarded Newton solve the Newton equations with
the R function solve(). In <code>snewton</code> a backtracking line search is used,
while in <code>snewtonm</code> we rely on a Marquardt stabilization.
</p>


<h3>Usage</h3>

<pre><code class="language-R">   snewton(par, fn, gr, hess, control = list(trace=0, maxit=500), ...)

   snewtm(par, fn, gr, hess, bds, control = list(trace=0, maxit=500))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>par</code></td>
<td>
<p>A numeric vector of starting estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fn</code></td>
<td>
<p>A function that returns the value of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gr</code></td>
<td>
<p>A function that returns the gradient of the objective at the
supplied set of parameters <code>par</code> using auxiliary data in ....
The first argument of <code>fn</code> must be <code>par</code>. This function 
returns the gradient as a numeric vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hess</code></td>
<td>
<p>A function to compute the Hessian matrix. This should be provided as a square,
symmetric matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bds</code></td>
<td>
<p>Result of <code>bmchk()</code> for the current problem. Contains lower and upper etc.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>

<p>An optional list of control settings.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments to be passed to <code>fn</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>snewtm</code> is intended to be called from <code>optimr()</code>.
</p>
<p>Functions <code>fn</code> must return a numeric value. <code>gr</code> must return a vector.
<code>hess</code> must return a matrix. 
The <code>control</code> argument is a list. See the code for <code>snewton.R</code> for completeness.
Some of the values that may be important for users are:
</p>

<dl>
<dt>trace</dt>
<dd>
<p>Set 0 (default) for no output, &gt; 0 for diagnostic output
(larger values imply more output).</p>
</dd>
<dt>watch</dt>
<dd>
<p>Set TRUE if the routine is to stop for user input (e.g., Enter)
after each iteration. Default is FALSE.</p>
</dd>
<dt>maxit</dt>
<dd>
<p>A limit on the number of iterations (default 500 + 2*n where n is
the number of parameters). This is the maximum number of gradient evaluations 
allowed.</p>
</dd>
<dt>maxfeval</dt>
<dd>
<p>A limit on the number of function evaluations allowed 
(default 3000 + 10*n).</p>
</dd>
<dt>eps</dt>
<dd>
<p> a tolerance used for judging small gradient norm (default = 1e-07).
a gradient norm smaller than (1 + abs(fmin))*eps*eps is considered small 
enough that a local optimum has been found, where fmin is the current 
estimate of the minimal function value. </p>
</dd>
<dt>acctol</dt>
<dd>
<p>To adjust the acceptable point tolerance (default 0.0001) in the test
( f &lt;= fmin + gradproj * steplength * acctol ). This test is used to ensure progress
is made at each iteration. </p>
</dd>
<dt>stepdec</dt>
<dd>
<p>Step reduction factor for backtrack line search (default 0.2)</p>
</dd>
<dt>defstep</dt>
<dd>
<p>Initial stepsize default (default 1)</p>
</dd>
<dt>reltest</dt>
<dd>
<p>Additive shift for equality test (default 100.0)</p>
</dd>
</dl>
<p>The (unconstrained) solver <code>snewtonmu</code> proved to be slower than the bounded solver
called without bounds, so has been withdrawn.
</p>
<p>The <code>snewton</code> safeguarded Newton uses a simple line search but no linear solution
stabilization and has demonstrated POOR performance and reliability. NOT recommended.
</p>


<h3>Value</h3>

<p>A list with components:
</p>

<dl>
<dt>par</dt>
<dd>
<p>The best set of parameters found.</p>
</dd>
<dt>value</dt>
<dd>
<p>The value of the objective at the best set of parameters found.</p>
</dd>
<dt>grad</dt>
<dd>
<p>The value of the gradient at the best set of parameters found. A vector.</p>
</dd>
<dt>hessian</dt>
<dd>
<p>The value of the Hessian at the best set of parameters found. A matrix.</p>
</dd>
<dt>counts</dt>
<dd>
<p>A vector of 4 integers giving number of Newton equation solutions, the number of function
evaluations, the number of gradient evaluations and the number of hessian evaluations.</p>
</dd>
<dt>message</dt>
<dd>
<p>A message giving some information on the status of the solution.</p>
</dd>
</dl>
<h3>References</h3>

 
<p>Nash, J C (1979, 1990) Compact Numerical Methods for Computers: Linear
Algebra and Function Minimisation, Bristol: Adam Hilger. Second
Edition, Bristol: Institute of Physics Publications.
</p>


<h3>See Also</h3>

<p><code>optim</code></p>


<h3>Examples</h3>

<pre><code class="language-R">#Rosenbrock banana valley function
f &lt;- function(x){
return(100*(x[2] - x[1]*x[1])^2 + (1-x[1])^2)
}
#gradient
gr &lt;- function(x){
return(c(-400*x[1]*(x[2] - x[1]*x[1]) - 2*(1-x[1]), 200*(x[2] - x[1]*x[1])))
}
#Hessian
h &lt;- function(x) {
a11 &lt;- 2 - 400*x[2] + 1200*x[1]*x[1]; a21 &lt;- -400*x[1]
return(matrix(c(a11, a21, a21, 200), 2, 2))
}

fg &lt;- function(x){ #function and gradient
  val &lt;- f(x)
  attr(val,"gradient") &lt;- gr(x)
  val
}
fgh &lt;- function(x){ #function and gradient
  val &lt;- f(x)
  attr(val,"gradient") &lt;- gr(x)
  attr(val,"hessian") &lt;- h(x)
  val
}

x0 &lt;- c(-1.2, 1)

sr &lt;- snewton(x0, fn=f, gr=gr, hess=h, control=list(trace=1))
print(sr)
# Call through optimr to get correct calling sequence, esp. with bounds
srm &lt;- optimr(x0, fn=f, gr=gr, hess=h, control=list(trace=1))
print(srm)

# bounds constrained example

lo &lt;- rep((min(x0)-0.1), 2)
up &lt;- rep((max(x0)+0.1), 2)
# Call through optimr to get correct calling sequence, esp. with bounds
srmb &lt;- optimr(x0, fn=f, gr=gr, hess=h, lower=lo, upper=up, control=list(trace=1))
proptimr(srmb)


#Example 2: Wood function
#
wood.f &lt;- function(x){
  res &lt;- 100*(x[1]^2-x[2])^2+(1-x[1])^2+90*(x[3]^2-x[4])^2+(1-x[3])^2+
    10.1*((1-x[2])^2+(1-x[4])^2)+19.8*(1-x[2])*(1-x[4])
  return(res)
}
#gradient:
wood.g &lt;- function(x){
  g1 &lt;- 400*x[1]^3-400*x[1]*x[2]+2*x[1]-2
  g2 &lt;- -200*x[1]^2+220.2*x[2]+19.8*x[4]-40
  g3 &lt;- 360*x[3]^3-360*x[3]*x[4]+2*x[3]-2
  g4 &lt;- -180*x[3]^2+200.2*x[4]+19.8*x[2]-40
  return(c(g1,g2,g3,g4))
}
#hessian:
wood.h &lt;- function(x){
  h11 &lt;- 1200*x[1]^2-400*x[2]+2;    h12 &lt;- -400*x[1]; h13 &lt;- h14 &lt;- 0
  h22 &lt;- 220.2; h23 &lt;- 0;    h24 &lt;- 19.8
  h33 &lt;- 1080*x[3]^2-360*x[4]+2;    h34 &lt;- -360*x[3]
  h44 &lt;- 200.2
  H &lt;- matrix(c(h11,h12,h13,h14,h12,h22,h23,h24,
                h13,h23,h33,h34,h14,h24,h34,h44),ncol=4)
  return(H)
}
#################################################
w0 &lt;- c(-3, -1, -3, -1)

wd &lt;- snewton(w0, fn=wood.f, gr=wood.g, hess=wood.h, control=list(trace=1))
print(wd)

# Call through optimr to get correct calling sequence, esp. with bounds
wdm &lt;- optimr(w0, fn=wood.f, gr=wood.g, hess=wood.h, control=list(trace=1))
print(wdm)

</code></pre>


</div>