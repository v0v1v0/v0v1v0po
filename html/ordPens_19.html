<div class="container">

<table style="width: 100%;"><tr>
<td>ordPCA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Penalized nonlinear PCA for ordinal variables
</h2>

<h3>Description</h3>

<p>This function performs nonlinear principal components analysis when the variables of interest have ordinal level scale using a second-order difference penalty.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ordPCA(H, p, lambda = c(1), maxit = 100, crit = 1e-7, qstart = NULL, 
       Ks = apply(H,2,max), constr = rep(FALSE, ncol(H)), trace = FALSE,
       CV = FALSE, k = 5, CVfit = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>H</code></td>
<td>
<p>a matrix or data frame of of integers 1,2,... giving the observed levels
of the ordinal variables; provides the data for the principal components analysis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>the number of principal components to be extracted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>a numeric value or a vector (in decreasing order) defining the amount of shrinkage; defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>the maximum number of iterations; defaults to 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>crit</code></td>
<td>
<p>convergence tolerance; defaults to 1e-7.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>qstart</code></td>
<td>
<p>optional list of quantifications for the initial linear PCA.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Ks</code></td>
<td>
<p>a vector containing the highest level of each variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>constr</code></td>
<td>
<p>a logical vector specifying whether monotonicity constraints should be applied to the variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>logical; if <code>TRUE</code>, tracing information on the progress of the optimization is produced in terms of VAF in each iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CV</code></td>
<td>
<p>a logical value indicating whether k-fold cross-validation should be performed in order to evaluate the
performance and/or select an optimal smoothing parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of folds to be specified; only if <code>CV</code> is set to <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CVfit</code></td>
<td>
<p>logical; to be specified only if <code>CV = TRUE</code>. If <code>CVfit = TRUE</code> and <code>lambda</code> is a vector of length &gt; 5, additional yes/no dialog appears;
if <code>FALSE</code>, only VAF values are provided (recommended); else, also lists of matrices of PCA results are produced and stored.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>In order to respect the ordinal scale of the data, principal components analysis is not applied to data matrix <code>H</code> itself, but to newly constructed variables by assigning numerical values – the quantifications – to the categories via penalized, optimal scaling/scoring. 
The calculation is done by alternately cycling through data scoring and PCA until convergence. 
</p>
<p>The penalty parameter controls the amount of shrinkage: For <code>lambda = 0</code>, purely nonlinear PCA via standard, optimal scaling is obtained. As <code>lambda</code> becomes very large, the quantifications are shrunken towars linearity, i.e., usual PCA is applied to levels 1,2,... ignoring the ordinal scale level of the variables. 
</p>
<p>Note that optimization starts with the first component of <code>lambda</code>. Thus, if <code>lambda</code> is not in decreasing order, the vector will be sorted internally and so will be corresponding results.
</p>
<p>In case of cross-validation, for each <code>lambda</code> the proportion of variance accounted for (VAF) is given for both the training and test data (see below).
</p>


<h3>Value</h3>

<p>A List with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>qs</code></td>
<td>
<p>a list of quantifications, if <code>lambda</code> is specified as a single value. Otherwise, a list of matrices,
each column corresponding to a certain <code>lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q</code></td>
<td>
<p>data matrix after scaling, if <code>lambda</code> is scalar. Otherwise, a list of matrices with each list entry corresponding to a certain <code>lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>matrix of factor values resulting from <code>prcomp</code>, if <code>lambda</code> is scalar. Otherwise, list of matrices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>loadings matrix as a result from <code>prcomp</code>, if <code>lambda</code> is scalar. 
Otherwise, list of matrices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>number of iterations used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pca</code></td>
<td>
<p>object of class <code>"prcomp"</code> returned by <code>prcomp</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>vector of VAF values in each iteration, if <code>lambda</code> is specified as a single value. Otherwise, a list of vectors, each entry corresponding to a certain <code>lambda</code> value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>VAFtrain</code></td>
<td>
<p>matrix with columns corresponding to <code>lambda</code> and rows corresponding to the folds <code>k</code>.
Contains corresponding proportions of variance accounted for (VAF) on the training data within cross-validation. VAF here is defined in terms of the proportion of variance explained by the first <code>p</code> PCs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>VAFtest</code></td>
<td>
<p>VAF matrix for the test data within cross-validation.</p>
</td>
</tr>
</table>
<p>If cross-validation is desired, the pca results are stored in a list called <code>fit</code> with each list entry corresponding to a certain fold. Within such a list entry, all sub entries can be accessed as described above. 
However, VAF values are stored in <code>VAFtrain</code> or <code>VAFtest</code> and can be accessed directly. 
</p>


<h3>Author(s)</h3>

<p>Aisouda Hoshiyar, Jan Gertheiss</p>


<h3>References</h3>

<p>Hoshiyar, A. (2020). <em>Analyzing Likert-type data using penalized non-linear principal components analysis</em>, in: Proceedings of the 35th International Workshop on Statistical Modelling, Vol. I, 337-340.
</p>
<p>Hoshiyar, A., H.A.L. Kiers, and J. Gertheiss (2021). <em>Penalized non-linear principal components analysis for ordinal variables with an application to international classification of functioning core sets</em>, British Journal of Mathematical and Statistical Psychology, 76, 353-371.
</p>
<p>Linting, M., J.J. Meulmann, A.J. von der Kooji, and P.J.F. Groenen (2007). 
<em>Nonlinear principal components analysis: Introduction and application</em>, 
Psychological Methods, 12, 336-358.
</p>


<h3>See Also</h3>

<p><code>prcomp</code>  
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
## load ICF data 
data(ICFCoreSetCWP)

# adequate coding to get levels 1,..., max 
H &lt;- ICFCoreSetCWP[, 1:67] + matrix(c(rep(1, 50), rep(5, 16), 1),
                                    nrow(ICFCoreSetCWP), 67,
                                    byrow = TRUE)
xnames &lt;- colnames(H)                                    
                                    
# nonlinear PCA
icf_pca1 &lt;- ordPCA(H, p = 2, lambda = c(5, 0.5, 0.0001), maxit = 1000, 
                   Ks = c(rep(5, 50), rep(9, 16), 5), 
                   constr = c(rep(TRUE, 50), rep(FALSE, 16), TRUE))

# estimated quantifications 
icf_pca1$qs[[55]]

plot(1:9, icf_pca1$qs[[55]][,1], type="b", 
xlab="category", ylab="quantification", col=1, main=xnames[55], 
ylim=range(c(icf_pca1$qs[[55]][,1],icf_pca1$qs[[55]][,2],icf_pca1$qs[[55]][,3])))
lines(icf_pca1$qs[[55]][,2], type = "b", col = 2, lty = 2, pch = 2, lwd=2)
lines(icf_pca1$qs[[55]][,3], type = "b", col = 3, lty = 3, pch = 3, lwd=2)

# compare VAF 
icf_pca2 &lt;- ordPCA(H, p = 2, lambda = c(5, 0.5, 0.0001), maxit = 1000, 
                   Ks = c(rep(5, 50), rep(9, 16), 5), 
                   constr = c(rep(TRUE, 50), rep(FALSE, 16), TRUE),
                   CV = TRUE, k = 5)
icf_pca2$VAFtest

## load ehd data 
require(psy)
data(ehd)

# recoding to get levels 1,..., max 
H &lt;- ehd + 1

# nonlinear PCA
ehd1 &lt;- ordPCA(H, p = 5, lambda = 0.5, maxit = 100,
               constr = rep(TRUE,ncol(H)),
               CV = FALSE)

# resulting PCA on the scaled variables
summary(ehd1$pca)

# plot quantifications
oldpar &lt;- par(mfrow = c(4,5))
for(j in 1:length(ehd1$qs))
  plot(1:5, ehd1$qs[[j]], type = "b", xlab = "level", ylab = "quantification",
  main = colnames(H)[j])
par(oldpar)

# include cross-validation
lambda &lt;- 10^seq(4,-4, by = -0.1)
set.seed(456)
cvResult &lt;- ordPCA(H, p = 5, lambda = lambda, maxit = 100,
                    constr = rep(TRUE,ncol(H)),
                    CV = TRUE, k = 5, CVfit = FALSE)
# optimal lambda                    
lambda[which.max(apply(cvResult$VAFtest,2,mean))]

## End(Not run)
</code></pre>


</div>