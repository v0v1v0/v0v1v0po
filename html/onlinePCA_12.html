<div class="container">

<table style="width: 100%;"><tr>
<td>incRpca.rc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Incremental PCA With Reduced Complexity</h2>

<h3>Description</h3>

<p>The incremental PCA is computed without rotating the updated projection space (Brand, 2002; Arora et al., 2012). Specifically, PCs are specified through a matrix of orthogonal vectors <code>Ut</code> that spans the PC space and a rotation matrix <code>Us</code> such that the PC matrix is <code>UtUs</code>. Given a new data vector, the PCA is updated by adding one column to <code>Ut</code> and recalculating the low-dimensional rotation matrix <code>Us</code>. This reduces complexity and helps preserving orthogonality. Eigenvalues are updated as the usual incremental PCA algorithm.</p>


<h3>Usage</h3>

<pre><code class="language-R">incRpca.rc(lambda, Ut, Us, x, n, f = 1/n, center, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>vector of eigenvalues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Ut</code></td>
<td>
<p>matrix of orthogonal vectors stored in columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Us</code></td>
<td>
<p>rotation matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>new data vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>sample size before observing <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>
<p>forgetting factor: a number in (0,1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>numerical tolerance for eigenvalues.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For large datasets, this algorithm is considerably faster than its counterpart <code>incRpca</code>, reducing the time complexity of each update from <code class="reqn">O(qd^2)</code> to <code class="reqn">O(qd + q^3)</code> flops with <code>d</code> the length of <code>x</code>. A consequence of not rotating the PC basis at each update is that the dimension of the PCA decomposition increases whenever a new observation vector is not entirely contained in the PC space. To keep the number of PCs and eigenvalues from getting too large, it is necessary to multiply the matrices <code class="reqn">U_t</code> and <code class="reqn">U_s</code> at regular time intervals so as to recover the individual PCs and retain only the largest ones.  
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr style="vertical-align: top;">
<td><code>values</code></td>
<td>
<p>updated eigenvalues in decreasing order.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Ut</code></td>
<td>
<p>updated projection space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Us</code></td>
<td>
<p>updated rotation matrix.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Arora et al. (2012). Stochastic Optimization for PCA and PLS.  <em>50th Annual Conference on Communication, Control, and Computing (Allerton).</em><br>
Brand, M. (2002). Incremental singular value decomposition of uncertain data with missing values. <em>European Conference on Computer Vision (ECCV).</em>
</p>


<h3>See Also</h3>

<p><code>incRpca, incRpca.block</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# Data generation
n &lt;- 400 # number of units
d &lt;- 10000 # number of variables
n0 &lt;- 200 # initial sample
q &lt;- 20 # required number of PCs
x &lt;- matrix(rnorm(n*d,sd=1/sqrt(d)),n,d) # data matrix
x &lt;- t(apply(x,1,cumsum)) # standard Brownian motion

# Initial PCA
# Initial PCA
xbar0 &lt;- colMeans(x[1:n0,])
pca0 &lt;- batchpca(x0c, q, center=xbar0, byrow=TRUE)

# Incremental PCA with rotation
xbar &lt;- xbar0
pca1 &lt;- pca0
system.time({
for (i in n0:(n-1)) {
	xbar &lt;- updateMean(xbar, x[i+1,], i)
	pca1 &lt;- incRpca(pca1$values, pca1$vectors, x[i+1,], i, center=xbar)
	}
})

# Incremental PCA without rotation
xbar &lt;- xbar0
pca2 &lt;- list(values=pca0$values, Ut=pca0$vectors, Us=diag(q))

system.time({
for (i in n0:(n-1)) {
	xbar &lt;- updateMean(xbar, x[i+1,], i)
	pca2 &lt;- incRpca.rc(pca2$values, pca2$Ut, pca2$Us, x[i+1,], 
		i, center = xbar)
	# Rotate the PC basis and reduce its size to q every k observations
	if (i %% q == 0 || i == n-1) 
		{ pca2$values &lt;- pca2$values[1:q]
		  pca2$Ut &lt;- pca2$Ut %*% pca2$Us[,1:q]
		  pca2$Us &lt;- diag(q)
	}
}
})

# Check that the results are identical
# Relative differences in eigenvalues
range(pca1$values/pca2$values-1)
# Cosines of angles between eigenvectors 
abs(colSums(pca1$vectors * pca2$Ut))

## End(Not run)
</code></pre>


</div>