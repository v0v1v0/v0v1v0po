<div class="container">

<table style="width: 100%;"><tr>
<td>ODRF</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Classification and Regression using Oblique Decision Random Forest</h2>

<h3>Description</h3>

<p>Classification and regression implemented by the oblique decision random forest. ODRF usually produces more accurate predictions than RF, but needs longer computation time.
</p>


<h3>Usage</h3>

<pre><code class="language-R">ODRF(X, ...)

## S3 method for class 'formula'
ODRF(
  formula,
  data = NULL,
  split = "auto",
  lambda = "log",
  NodeRotateFun = "RotMatPPO",
  FunDir = getwd(),
  paramList = NULL,
  ntrees = 100,
  storeOOB = TRUE,
  replacement = TRUE,
  stratify = TRUE,
  ratOOB = 1/3,
  parallel = TRUE,
  numCores = Inf,
  MaxDepth = Inf,
  numNode = Inf,
  MinLeaf = 5,
  subset = NULL,
  weights = NULL,
  na.action = na.fail,
  catLabel = NULL,
  Xcat = 0,
  Xscale = "Min-max",
  TreeRandRotate = FALSE,
  ...
)

## Default S3 method:
ODRF(
  X,
  y,
  split = "auto",
  lambda = "log",
  NodeRotateFun = "RotMatPPO",
  FunDir = getwd(),
  paramList = NULL,
  ntrees = 100,
  storeOOB = TRUE,
  replacement = TRUE,
  stratify = TRUE,
  ratOOB = 1/3,
  parallel = TRUE,
  numCores = Inf,
  MaxDepth = Inf,
  numNode = Inf,
  MinLeaf = 5,
  subset = NULL,
  weights = NULL,
  na.action = na.fail,
  catLabel = NULL,
  Xcat = 0,
  Xscale = "Min-max",
  TreeRandRotate = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>An n by d numeric matrix (preferable) or data frame.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional parameters to be passed to the low level function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>Object of class <code>formula</code> with a response describing the model to fit. If this is a data frame, it is taken as the model frame. (see <code>model.frame</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>Training data of class <code>data.frame</code> containing variables named in the formula. If <code>data</code> is missing it is obtained from the current environment by <code>formula</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>split</code></td>
<td>
<p>The criterion used for splitting the nodes. "entropy": information gain and "gini": gini impurity index for classification; "mse": mean square error for regression;
'auto' (default): If the response in <code>data</code> or <code>y</code> is a factor, "gini" is used, otherwise regression is assumed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The argument of <code>split</code> is used to determine the penalty level of the partition criterion. Three options are provided including, <code>lambda=0</code>: no penalty; <code>lambda=2</code>: AIC penalty; <code>lambda='log'</code> (Default): BIC penalty. In Addition, lambda can be any value from 0 to n (training set size).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>NodeRotateFun</code></td>
<td>
<p>Name of the function of class <code>character</code> that implements a linear combination of predictors in the split node.
including </p>

<ul>
<li>
<p>"RotMatPPO": projection pursuit optimization model (<code>PPO</code>), see <code>RotMatPPO</code> (default, model="PPR").
</p>
</li>
<li>
<p>"RotMatRF": single feature similar to Random Forest, see <code>RotMatRF</code>.
</p>
</li>
<li>
<p>"RotMatRand": random rotation, see <code>RotMatRand</code>.
</p>
</li>
<li>
<p>"RotMatMake": users can define this function, for details see <code>RotMatMake</code>.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FunDir</code></td>
<td>
<p>The path to the <code>function</code> of the user-defined <code>NodeRotateFun</code> (default current working directory).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>paramList</code></td>
<td>
<p>List of parameters used by the functions <code>NodeRotateFun</code>. If left unchanged, default values will be used, for details see <code>defaults</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntrees</code></td>
<td>
<p>The number of trees in the forest (default 100).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>storeOOB</code></td>
<td>
<p>If TRUE then the samples omitted during the creation of a tree are stored as part of the tree (default TRUE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>replacement</code></td>
<td>
<p>if TRUE then n samples are chosen, with replacement, from training data (default TRUE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stratify</code></td>
<td>
<p>If TRUE then class sample proportions are maintained during the random sampling. Ignored if replacement = FALSE (default TRUE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ratOOB</code></td>
<td>
<p>Ratio of 'out-of-bag' (default 1/3).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>Parallel computing or not (default TRUE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numCores</code></td>
<td>
<p>Number of cores to be used for parallel computing (default <code>Inf</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MaxDepth</code></td>
<td>
<p>The maximum depth of the tree (default <code>Inf</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>numNode</code></td>
<td>
<p>Number of nodes that can be used by the tree (default <code>Inf</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MinLeaf</code></td>
<td>
<p>Minimal node size (Default 5).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>An index vector indicating which rows should be used. (NOTE: If given, this argument must be named.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Vector of non-negative observational weights; fractional weights are allowed (default NULL).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>A function to specify the action to be taken if NAs are found. (NOTE: If given, this argument must be named.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>catLabel</code></td>
<td>
<p>A category labels of class <code>list</code> in predictors. (default NULL, for details see Examples)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xcat</code></td>
<td>
<p>A class <code>vector</code> is used to indicate which predictor is the categorical variable. The default Xcat=0 means that no special treatment is given to category variables.
When Xcat=NULL, the predictor x that satisfies the condition "<code>(length(table(x))&lt;10) &amp; (length(x)&gt;20)</code>" is judged to be a category variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xscale</code></td>
<td>
<p>Predictor standardization methods. " Min-max" (default), "Quantile", "No" denote Min-max transformation, Quantile transformation and No transformation respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TreeRandRotate</code></td>
<td>
<p>If or not to randomly rotate the training data before building the tree (default FALSE, see <code>RandRot</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A response vector of length n.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An object of class ODRF Containing a list components:
</p>

<ul>
<li>
<p><code>call</code>: The original call to ODRF.
</p>
</li>
<li>
<p><code>terms</code>: An object of class <code>c("terms", "formula")</code> (see <code>terms.object</code>) summarizing the formula. Used by various methods, but typically not of direct relevance to users.
</p>
</li>
<li>
<p><code>split</code>, <code>Levels</code> and <code>NodeRotateFun</code> are important parameters for building the tree.
</p>
</li>
<li>
<p><code>predicted</code>: the predicted values of the training data based on out-of-bag samples.
</p>
</li>
<li>
<p><code>paramList</code>: Parameters in a named list to be used by <code>NodeRotateFun</code>.
</p>
</li>
<li>
<p><code>oobErr</code>: 'out-of-bag' error for forest, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>
</li>
<li>
<p><code>oobConfusionMat</code>: 'out-of-bag' confusion matrix for forest.
</p>
</li>
<li>
<p><code>structure</code>: Each tree structure used to build the forest. </p>

<ul>
<li>
<p><code>oobErr</code>: 'out-of-bag' error for tree, misclassification rate (MR) for classification or mean square error (MSE) for regression.
</p>
</li>
<li>
<p><code>oobIndex</code>: Which training data to use as 'out-of-bag'.
</p>
</li>
<li>
<p><code>oobPred</code>: Predicted value for 'out-of-bag'.
</p>
</li>
<li>
<p><code>others</code>: Same tree structure return value as <code>ODT</code>.
</p>
</li>
</ul>
</li>
<li>
<p><code>data</code>: The list of data related parameters used to build the forest.
</p>
</li>
<li>
<p><code>tree</code>: The list of tree related parameters used to build the tree.
</p>
</li>
<li>
<p><code>forest</code>: The list of forest related parameters used to build the forest.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Yu Liu and Yingcun Xia
</p>


<h3>References</h3>

<p>Zhan, H., Liu, Y., &amp; Xia, Y. (2022). Consistency of The Oblique Decision Tree and Its Random Forest. arXiv preprint arXiv:2211.12653.
</p>
<p>Tomita, T. M., Browne, J., Shen, C., Chung, J., Patsolic, J. L., Falk, B., ... &amp; Vogelstein, J. T. (2020). Sparse projection oblique randomer forests. Journal of machine learning research, 21(104).
</p>


<h3>See Also</h3>

<p><code>online.ODRF</code> <code>prune.ODRF</code> <code>predict.ODRF</code> <code>print.ODRF</code> <code>Accuracy</code> <code>VarImp</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Classification with Oblique Decision Randome Forest.
data(seeds)
set.seed(221212)
train &lt;- sample(1:209, 80)
train_data &lt;- data.frame(seeds[train, ])
test_data &lt;- data.frame(seeds[-train, ])
forest &lt;- ODRF(varieties_of_wheat ~ ., train_data,
  split = "entropy",parallel = FALSE, ntrees = 50
)
pred &lt;- predict(forest, test_data[, -8])
# classification error
(mean(pred != test_data[, 8]))

# Regression with Oblique Decision Randome Forest.
data(body_fat)
set.seed(221212)
train &lt;- sample(1:252, 80)
train_data &lt;- data.frame(body_fat[train, ])
test_data &lt;- data.frame(body_fat[-train, ])
forest &lt;- ODRF(Density ~ ., train_data,
  split = "mse", parallel = FALSE,
  NodeRotateFun = "RotMatPPO", paramList = list(model = "Log", dimProj = "Rand")
)
pred &lt;- predict(forest, test_data[, -1])
# estimation error
mean((pred - test_data[, 1])^2)


### Train ODRF on one-of-K encoded categorical data ###
# Note that the category variable must be placed at the beginning of the predictor X
# as in the following example.
set.seed(22)
Xcol1 &lt;- sample(c("A", "B", "C"), 100, replace = TRUE)
Xcol2 &lt;- sample(c("1", "2", "3", "4", "5"), 100, replace = TRUE)
Xcon &lt;- matrix(rnorm(100 * 3), 100, 3)
X &lt;- data.frame(Xcol1, Xcol2, Xcon)
Xcat &lt;- c(1, 2)
catLabel &lt;- NULL
y &lt;- as.factor(sample(c(0, 1), 100, replace = TRUE))

forest &lt;- ODRF(y ~ X, split = "entropy", Xcat = NULL, parallel = FALSE)

head(X)
#&gt;   Xcol1 Xcol2          X1         X2          X3
#&gt; 1     B     5 -0.04178453  2.3962339 -0.01443979
#&gt; 2     A     4 -1.66084623 -0.4397486  0.57251733
#&gt; 3     B     2 -0.57973333 -0.2878683  1.24475578
#&gt; 4     B     1 -0.82075051  1.3702900  0.01716528
#&gt; 5     C     5 -0.76337897 -0.9620213  0.25846351
#&gt; 6     A     5 -0.37720294 -0.1853976  1.04872159

# one-of-K encode each categorical feature and store in X1
numCat &lt;- apply(X[, Xcat, drop = FALSE], 2, function(x) length(unique(x)))
# initialize training data matrix X1
X1 &lt;- matrix(0, nrow = nrow(X), ncol = sum(numCat))
catLabel &lt;- vector("list", length(Xcat))
names(catLabel) &lt;- colnames(X)[Xcat]
col.idx &lt;- 0L
# convert categorical feature to K dummy variables
for (j in seq_along(Xcat)) {
  catMap &lt;- (col.idx + 1):(col.idx + numCat[j])
  catLabel[[j]] &lt;- levels(as.factor(X[, Xcat[j]]))
  X1[, catMap] &lt;- (matrix(X[, Xcat[j]], nrow(X), numCat[j]) ==
    matrix(catLabel[[j]], nrow(X), numCat[j], byrow = TRUE)) + 0
  col.idx &lt;- col.idx + numCat[j]
}
X &lt;- cbind(X1, X[, -Xcat])
colnames(X) &lt;- c(paste(rep(seq_along(numCat), numCat), unlist(catLabel),
  sep = "."
), "X1", "X2", "X3")

# Print the result after processing of category variables.
head(X)
#&gt;   1.A 1.B 1.C 2.1 2.2 2.3 2.4 2.5          X1         X2          X3
#&gt; 1   0   1   0   0   0   0   0   1 -0.04178453  2.3962339 -0.01443979
#&gt; 2   1   0   0   0   0   0   1   0 -1.66084623 -0.4397486  0.57251733
#&gt; 3   0   1   0   0   1   0   0   0 -0.57973333 -0.2878683  1.24475578
#&gt; 4   0   1   0   1   0   0   0   0 -0.82075051  1.3702900  0.01716528
#&gt; 5   0   0   1   0   0   0   0   1 -0.76337897 -0.9620213  0.25846351
#&gt; 6   1   0   0   0   0   0   0   1 -0.37720294 -0.1853976  1.04872159
catLabel
#&gt; $Xcol1
#&gt; [1] "A" "B" "C"
#&gt;
#&gt; $Xcol2
#&gt; [1] "1" "2" "3" "4" "5"


forest &lt;- ODRF(X, y,
  split = "gini", Xcat = c(1, 2),
  catLabel = catLabel, parallel = FALSE
)


</code></pre>


</div>