<div class="container">

<table style="width: 100%;"><tr>
<td>completions_request</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>API completions: create request</h2>

<h3>Description</h3>

<p>To get more details, visit https://platform.openai.com/docs/api-reference/completions/create
</p>


<h3>Usage</h3>

<pre><code class="language-R">completions_request(
  model,
  prompt,
  suffix = NULL,
  max_tokens = NULL,
  temperature = NULL,
  top_p = NULL,
  n = NULL,
  stream = NULL,
  logprobs = NULL,
  echo = NULL,
  stop = NULL,
  presence_penalty = NULL,
  frequency_penalty = NULL,
  best_of = NULL,
  user = NULL,
  api_key = api_get_key()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>string, ID of the model to use. You can use the list models
(https://platform.openai.com/docs/api-reference/models/list)
API to see all of your available models, or see our model overview
(https://platform.openai.com/docs/models/overview)
for descriptions of them.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prompt</code></td>
<td>
<p>API endpoint parameter</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>suffix</code></td>
<td>
<p>string/NULL, the suffix that comes after a completion
of inserted text.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_tokens</code></td>
<td>
<p>integer, the maximum number of tokens
(https://platform.openai.com/tokenizer) to generate
in the completion. The token count of your prompt plus max_tokens cannot
exceed the model's context length.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>temperature</code></td>
<td>
<p>double, what sampling temperature to use, between 0 and 2.
Higher values like 0.8 will make the output more random,
while lower values like 0.2 will make it more focused and deterministic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>top_p</code></td>
<td>
<p>double, an alternative to sampling with temperature,
called nucleus sampling, where the model considers the results of the tokens
with top_p probability mass. So 0.1 means only the tokens comprising
the top 10% probability mass are considered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n</code></td>
<td>
<p>integer, How many completions to generate for each prompt.
Note: Because this parameter generates many completions,
it can quickly consume your token quota. Use carefully
and ensure that you have reasonable settings for 'max_tokens' and 'stop'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stream</code></td>
<td>
<p>flag, Whether to stream back partial progress. If set,
tokens will be sent as data-only server-sent events as they become available,
with the stream terminated by a data: '[DONE]' message.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logprobs</code></td>
<td>
<p>integer, Include the log probabilities on the logprobs most
likely tokens, as well the chosen tokens. For example, if logprobs is 5,
the API will return a list of the 5 most likely tokens. The API will always
return the logprob of the sampled token, so there may be up to logprobs+1
elements in the response.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>echo</code></td>
<td>
<p>logical, echo back the prompt in addition to the completion</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop</code></td>
<td>
<p>string or array, up to 4 sequences where the API will stop
generating further tokens. The returned text will not contain the stop sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>presence_penalty</code></td>
<td>
<p>double, Number between -2.0 and 2.0.
Positive values penalize new tokens based on whether they appear in the text
so far, increasing the model's likelihood to talk about new topics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>frequency_penalty</code></td>
<td>
<p>double, Number between -2.0 and 2.0.
Positive values penalize new tokens based on their existing frequency
in the text so far, decreasing the model's likelihood to repeat the same line
verbatim.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best_of</code></td>
<td>
<p>integer, Generates best_of completions server-side and returns
the "best" (the one with the highest log probability per token). Results
cannot be streamed.
When used with n, best_of controls the number of candidate completions
and n specifies how many to return â€“ best_of must be greater than n.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>user</code></td>
<td>
<p>string, A unique identifier representing your end-user,
which can help OpenAI to monitor and detect abuse</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>api_key</code></td>
<td>
<p>string, OpenAI API key
(see https://platform.openai.com/account/api-keys)</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>content of the httr response object
or SimpleError (conditions) enhanced with
two additional fields: 'status_code' (response$status_code)
and 'message_long' (built on response content)
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
  prompt &lt;- "x=1, y=2, z=x*y, z=?"
  res_content &lt;- completions_request(
    model = "text-davinci-003",
    prompt = prompt
  )
  if (!is_error(res_content)) {
    answer &lt;- completions_fetch_text(res_content)
    print(answer)
  }

## End(Not run)

</code></pre>


</div>