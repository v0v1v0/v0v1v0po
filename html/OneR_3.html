<div class="container">

<table style="width: 100%;"><tr>
<td>eval_model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Classification Evaluation function</h2>

<h3>Description</h3>

<p>Function for evaluating a OneR classification model. Prints confusion matrices with prediction vs. actual in absolute and relative numbers. Additionally it gives the accuracy, error rate as well as the error rate reduction versus the base rate accuracy together with a p-value.
</p>


<h3>Usage</h3>

<pre><code class="language-R">eval_model(prediction, actual, dimnames = c("Prediction", "Actual"),
  zero.print = "0")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>prediction</code></td>
<td>
<p>vector which contains the predicted values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>actual</code></td>
<td>
<p>data frame which contains the actual data. When there is more than one column the last last column is taken. A single vector is allowed too.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dimnames</code></td>
<td>
<p>character vector of printed dimnames for the confusion matrices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>zero.print</code></td>
<td>
<p>character specifying how zeros should be printed; for sparse confusion matrices, using "." can produce more readable results.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Error rate reduction versus the base rate accuracy is calculated by the following formula:<br><br><code class="reqn">(Accuracy(Prediction) - Accuracy(Baserate)) / (1 - Accuracy(Baserate))</code>,<br><br>
giving a number between 0 (no error reduction) and 1 (no error).<br><br>
In some borderline cases when the model is performing worse than the base rate negative numbers can result. This shows that something is seriously wrong with the model generating this prediction.<br><br>
The provided p-value gives the probability of obtaining a distribution of predictions like this (or even more unambiguous) under the assumption that the real accuracy is equal to or lower than the base rate accuracy.
More technicaly it is derived from a one-sided binomial test with the alternative hypothesis that the prediction's accuracy is bigger than the base rate accuracy.
Loosly speaking a low p-value (&lt; 0.05) signifies that the model really is able to give predictions that are better than the base rate.
</p>


<h3>Value</h3>

<p>Invisibly returns a list with the number of correctly classified and total instances and a confusion matrix with the absolute numbers.
</p>


<h3>Author(s)</h3>

<p>Holger von Jouanne-Diedrich
</p>


<h3>References</h3>

<p><a href="https://github.com/vonjd/OneR">https://github.com/vonjd/OneR</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data &lt;- iris
model &lt;- OneR(data)
summary(model)
prediction &lt;- predict(model, data)
eval_model(prediction, data)
</code></pre>


</div>