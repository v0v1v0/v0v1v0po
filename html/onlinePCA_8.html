<div class="container">

<table style="width: 100%;"><tr>
<td>ghapca</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generalized Hebbian Algorithm for PCA</h2>

<h3>Description</h3>

<p>Online PCA with the GHA of Sanger (1989).</p>


<h3>Usage</h3>

<pre><code class="language-R">ghapca(lambda, U, x, gamma, q = length(lambda), center, sort = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>optional vector of eigenvalues.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>U</code></td>
<td>
<p>matrix of eigenvectors (PC) stored in columns.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>new data vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>vector of gain parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>number of eigenvectors to compute.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>
<p>optional centering vector for <code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sort</code></td>
<td>
<p>Should the new eigenpairs be sorted?</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The vector <code>gamma</code> determines the weight placed on the new data in updating each eigenvector (the first coefficient of <code>gamma</code> corresponds to the first eigenvector, etc). It can be specified as a single positive number or as a vector of length <code>ncol(U)</code>. Larger values of <code>gamma</code> place more weight on <code>x</code> and less on <code>U</code>. A common choice for (the components of) <code>gamma</code> is of the form <code>c/n</code>, with <code>n</code> the sample size and <code>c</code> a suitable positive constant. <br>
If <code>sort</code> is TRUE and <code>lambda</code> is not missing, the updated eigenpairs are sorted by decreasing eigenvalue. Otherwise, they are not sorted.    
</p>


<h3>Value</h3>

<p>A list with components  
</p>
<table>
<tr style="vertical-align: top;">
<td><code>values</code></td>
<td>
<p>updated eigenvalues or NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vectors</code></td>
<td>
<p>updated eigenvectors.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Sanger (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network. <em>Neural Networks.</em>
</p>


<h3>See Also</h3>

<p><code>sgapca</code>, <code>snlpca</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Initialization
n &lt;- 1e4  # sample size
n0 &lt;- 5e3 # initial sample size
d &lt;- 10   # number of variables
q &lt;- d # number of PC
x &lt;- matrix(runif(n*d), n, d)
x &lt;- x %*% diag(sqrt(12*(1:d)))
# The eigenvalues of X are close to 1, 2, ..., d
# and the corresponding eigenvectors are close to 
# the canonical basis of R^d

## GHA PCA
pca &lt;- princomp(x[1:n0,])
xbar &lt;- pca$center
pca &lt;- list(values=pca$sdev[1:q]^2, vectors=pca$loadings[,1:q])
for (i in (n0+1):n) {
  xbar &lt;- updateMean(xbar, x[i,], i-1)
  pca &lt;- ghapca(pca$values, pca$vectors, x[i,], 2/i, q, xbar)
}
</code></pre>


</div>